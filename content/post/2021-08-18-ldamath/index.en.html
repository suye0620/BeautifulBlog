---
title: LDA主题模型数学推导
author: Ye.S
date: '2021-08-18'
slug: [ldamath]
categories:
  - 文章
tags:
  - math
  - NLP
  
cover: /img/LDAMath/LDAcover.jpg
output:
  blogdown::html_page:
    toc: true
  
summary: "关于LDA主题模型的数学推导过程"
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#前言">前言</a></li>
<li><a href="#统计文本建模">统计文本建模</a></li>
<li><a href="#unigram-model一元语法模型">Unigram Model(一元语法模型)</a>
<ul>
<li><a href="#unigram-model的概念">Unigram Model的概念</a></li>
<li><a href="#补充1多项分布">补充1：多项分布</a></li>
<li><a href="#补充2先验分布后验分布共轭分布">补充2：先验分布、后验分布、共轭分布</a></li>
</ul></li>
</ul>
</div>

<div id="前言" class="section level2">
<h2>前言</h2>
<p><span class="math inline">\(\quad\)</span>在机器学习术语中，LDA既指Linear Discriminant Analysis(线性判别分析)，又指Latent Dirichlet Allocation(隐含迪利克雷分布)。在这篇文章中，我们研究的将是后者。LDA是在我大二下第一次做市场调 查大赛时进入我的视线的。当时应用此法做主题-关键词提取时，我虽然感觉其原理晦涩难懂、读起来脑壳 痛，但是从做出来的效果来看用此法进行简单的文本关键词抽取是非常省事的。之后，我在完成数据科学 概论、Python数据分析这两门课程时，又反复琢磨此法原理，但还是徒劳无功。现在，我决心从头到尾梳理 一遍此法的数学原理，将其功力公诸于世。</p>
<center>
<p><img alt = '菜鸟每天飞过' src = "pic/cainiao.jpg" width=40%></p>
</center>
</div>
<div id="统计文本建模" class="section level2">
<h2>统计文本建模</h2>
<p><span class="math inline">\(\quad\)</span>我们日常生活中产生了大量的文本，如果每一个文本存储为一篇文档，那么每篇文档从人的观察来说就是有序的词的序列<span class="math inline">\(d = (w_1,w_2,...,w_n)\)</span>。</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="pic/pic_1-1.jpg" alt="包含m篇文档的语料库" width="266" />
<p class="caption">
图 1: 包含m篇文档的语料库
</p>
</div>
<p><span class="math inline">\(\quad\)</span>统计文本建模的目的就是探寻这些观察到语料库中的词序列是如何生成的。LDA就是一种优秀的统计文本建模方法。在讨论LDA模型前，我们先研究一下文本主题模型——Unigram Model以及LDA的前身PLSA。</p>
</div>
<div id="unigram-model一元语法模型" class="section level2">
<h2>Unigram Model(一元语法模型)</h2>
<div id="unigram-model的概念" class="section level3">
<h3>Unigram Model的概念</h3>
<p><span class="math inline">\(\quad\)</span>假设我们的词典(语料库corpus)中一共有<span class="math inline">\(V\)</span>个词<span class="math inline">\(v_1,v_2,...,v_V\)</span>，那么简单的Unigram Model就认为文本是基于如下规则产生的。</p>
<table>
<thead>
<tr class="header">
<th align="left">Unigram Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1. 假设有一个骰子，这个骰子有<span class="math inline">\(V\)</span>个面，每个面对应一个词，各个面朝上的概率不一；</td>
</tr>
<tr class="even">
<td align="left">2. 每抛掷一次骰子，抛出的面就对应地产生一个词；如果一篇文档中有<span class="math inline">\(n\)</span>个词，产生一篇文档就要独立地抛掷<span class="math inline">\(n\)</span>次骰子产生这<span class="math inline">\(n\)</span>个词。</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\quad\)</span>这个骰子的各个面的概率记为<span class="math inline">\(\overrightarrow{p} = (p_1,p_2,...,p_V)\)</span>，其中<span class="math inline">\(\sum_{k = 1}^Vp_k = 1\)</span>，所以每次投掷骰子的过程类似于一个抛硬币的贝努利实验，只是贝努利实验中我们抛的是一个“两面的骰子”，而此处抛掷的是一个<span class="math inline">\(V\)</span>面的骰子，我们把抛这个<span class="math inline">\(V\)</span>面骰子的实验记为<span class="math inline">\(w \sim Mult(V,\overrightarrow{p})\)</span>。</p>
<p>说明：Multinomial distribution——多项分布，<span class="math inline">\(w \sim Mult(V,\overrightarrow{p})\)</span>的含义为，在骰子各面或者说语料库中各个词出现的概率给定的情况下(<span class="math inline">\(\overrightarrow{p}\)</span>)，我们进行一次生成word的实验，则这个word符合多项分布。</p>
</div>
<div id="补充1多项分布" class="section level3">
<h3>补充1：多项分布</h3>
<p><span class="math inline">\(\quad\)</span>多项分布，Wiki百科的定义是这样的：假设一个人做了一个实验，他从一个装有<span class="math inline">\(k\)</span>种不同颜色球的袋子里有放回地摸出<span class="math inline">\(n\)</span>个球，对于同一种颜色的球，被摸到的概率都是相等的。然后，我们用<span class="math inline">\(X_i\)</span>表示摸到第<span class="math inline">\(i\)</span>(<span class="math inline">\(1 \leq i \leq k\)</span>)种颜色球的个数，用<span class="math inline">\(p_i\)</span>表示实验中摸到第<span class="math inline">\(i\)</span>种颜色球的概率，则这个实验的结果(一系列<span class="math inline">\(X_i\)</span>)满足多项分布，有：</p>
<!-- 公式区 -->
<p><span class="math display">\[\begin{align*}

&amp; P(X_1 = x_1,X_2=x_2,…,X_k = x_k) \\
&amp; =
\begin{cases}
\frac{n!}{x_1!\cdots x_k!}p_1^{x_1}\times \cdots \times p_k^{x_k} &amp;, &amp; \text{when $\sum_{i = 1}^k x_i = n$,}\\
0 &amp;, &amp; \text{otherwise.}
\end{cases}\\

&amp; \text{其中$x_1,X_2=x_2,…,X_k = x_k$均为非负整数}
\end{align*}\]</span></p>
<p><span class="math inline">\(\quad\)</span>对于一篇文档<span class="math inline">\(d = \overrightarrow{w} = (w_1,w_2, \cdots ,w_n)\)</span>，该文档被生成的概率就是
<span class="math display">\[p(\overrightarrow{w}) = p(w_1,w_2, \cdots ,w_n) = p(w_1)p(w_2)\cdots p(w_n)\]</span>
而文档和文档之间我们认为是独立的，所以如果语料中有多篇文档<span class="math inline">\(W = (\overrightarrow{w_1},\overrightarrow{w_2}, \cdots,\overrightarrow{w_m})\)</span>(用向量表示一个corpus)，则该语料的概率是
<span class="math display">\[
p(W) = p(\overrightarrow{w_1})p(\overrightarrow{w_2})\cdots p(\overrightarrow{w_n})
\]</span></p>
<p><span class="math inline">\(\quad\)</span>在Unigram Model中，我们假设了文档之间是独立可交换的，而文档中的词也是独立可交换的，所以一篇文档相当于一个袋子，里面装了一些词，而词的顺序信息就无关紧要了，这样的模型也称为词袋模型(Bag-of-words)。</p>
<p><span class="math inline">\(\quad\)</span>假设语料中总的词频是<span class="math inline">\(N\)</span>(所有词的词频加总，我的理解是我们做上述多项分布实验的次数)，在所有的<span class="math inline">\(N\)</span>个词中，如果我们关注每个词<span class="math inline">\(w_i\)</span>的发生次数<span class="math inline">\(n_i\)</span>(词频)，那么<span class="math inline">\(\overrightarrow{n} = (n_1,n_2, \cdots ,n_V),其中\sum_{k = 1}^{V}n_k = N\)</span>就是一个多项分布：</p>
<!-- 公式区 -->
<p><span class="math display">\[\begin{align*}
\overrightarrow{n} \sim Mult(N,\overrightarrow{p})\\

p(\overrightarrow{n}) 
= \binom{N}{\overrightarrow{n}}\prod_{k = 1}^{V}p_k^{n_k}\\
\end{align*}\]</span></p>
<p><span class="math inline">\(\quad\)</span>这里关于<span class="math inline">\(\binom{N}{\overrightarrow{n}}\)</span>，我想多说几句。首先这种写法我只在《LDA数学八卦》上见过，个人觉得有点迷惑。所以根据多项分布的计算公式，个人觉得它是这个含义：</p>
<!-- 公式区 -->
<p><span class="math display">\[\begin{align*}

\binom{N}{\overrightarrow{n}}
&amp;= C_N^{n_1} C_{N-{n_1}}^{n_2} C_{N-{n_1}-{n_2}}^{n_3} \cdots \\
&amp;= \frac{N!}{n_1!n_2! \cdots n_V!}
\end{align*}\]</span></p>
<p><span class="math inline">\(\quad\)</span>承上面的假设，并且语料库中各文档(设有m篇)相互独立，此时语料库的概率<span class="math inline">\(p(\overrightarrow{w})\)</span>是：
<span class="math display">\[
p(W) = p(\overrightarrow{w_1})p(\overrightarrow{w_2}) \cdots p(\overrightarrow{w_m}) =  \prod_{k=1}^{V}p_k^{n_k} \\
注：因为词袋模型忽略词序，\\
故直接概率连乘。
\]</span></p>
<p><span class="math inline">\(\quad\)</span>现在，我们的任务就是用手中的文本(样本)估计Unigram模型中的参数<span class="math inline">\(\overrightarrow{p}\)</span>，也就是计算语料库中各个词出现的概率是多大。按照频率派的观点，使用MLE最大化<span class="math inline">\(P(W)\)</span>。下面给出一个详细的推导步骤：</p>
<ol style="list-style-type: decimal">
<li>取似然函数为
<span class="math display">\[
L(\overrightarrow{p})
= \prod_{k=1}^{V}p_k^{n_k}\\
\]</span></li>
<li>对数似然函数为</li>
</ol>
<p><span class="math display">\[
lnL(\overrightarrow{p}) = \sum_{k=1}^{V}n_kln(p_k)
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>要使<span class="math inline">\(L(\overrightarrow{p})\)</span>最大，即要使<span class="math inline">\(lnL(\overrightarrow{p})\)</span>最大。而<span class="math inline">\(lnL(\overrightarrow{p})\)</span>这个多元函数是有约束条件的：<span class="math inline">\(\sum_{k=1}^V n_k = N\)</span>和<span class="math inline">\(\sum_{k=1}^{V}p_k = 1\)</span>，故用拉格朗日乘数法求之：</li>
</ol>
<!-- 公式区 -->
<p><span class="math display">\[\begin{align*}
令F(\overrightarrow{p},\lambda) = \sum_{k=1}^{V}n_k ln(p_k) - \lambda(\sum_{k=1}^{V}p_k -1)\\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
再令
\begin{cases}
\frac{\partial F}{\partial p_k}
&amp;=&amp; \frac{n_k}{p_k} - \lambda =0\\
\frac{\partial F}{\partial \lambda}
&amp;=&amp; \sum_{k=1}^{V}p_k -1 =0
\end{cases}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
得
\begin{cases}
p_k = \frac{n_k}{\lambda}\\
\sum_{k=1}^{V}p_k = 1
\end{cases}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
故
\sum_{k=1}^{V}p_k = 
\sum_{k=1}^{V}\frac{n_k}{\lambda} = 
\frac{\sum_{k=1}^{V}n_k}{\lambda} = 
\frac{N}{\lambda} = 1
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
即有
\lambda = N\\
\hat{p_k} = \frac{n_k}{\lambda} = \frac{n_k}{N}
\end{align*}\]</span></p>
</div>
<div id="补充2先验分布后验分布共轭分布" class="section level3">
<h3>补充2：先验分布、后验分布、共轭分布</h3>
<p><span class="math inline">\(\quad\)</span>以上模型，均基于频率学派观点。贝叶斯统计学派会对此有不同的意见。在他们看来，一切参数都是随机变量，上述Unigram Model中的<span class="math inline">\(\overrightarrow{p}\)</span>不是唯一固定的，它是一个随机变量。基于这样的想法，我们对Unigram Model的规则进行修改：</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">贝叶斯版本的Unigram Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1. 有一个装有无穷多个骰子的坛子，每个骰子都有<span class="math inline">\(V\)</span>个面，每个面对应一个词，各个面朝上的概率不一。并且各个骰子代表的<span class="math inline">\(\overrightarrow{p}\)</span>不一样；</td>
</tr>
<tr class="even">
<td align="left">2. 我们从坛子里抽出一个骰子，即选择一个<span class="math inline">\(\overrightarrow{p}\)</span>，然后用这个骰子不断地抛，直至产生了语料中的所有词(即达到了上面那个语料库词频向量<span class="math inline">\(\overrightarrow{n}\)</span>)。</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(\quad\)</span>这个坛子里面，骰子可以是无穷多个，有些类型的骰子多，有些类型的骰子少。所以从概率分布的角度看，坛子里面骰子代表的<span class="math inline">\(\overrightarrow{p}\)</span>服从一个概率分布<span class="math inline">\(P(\overrightarrow{p})\)</span>，这个分布称为参数<span class="math inline">\(\overrightarrow{p}\)</span>的先验分布。</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="pic/Unigram1.jpg" alt="两种观点下Unigram Model的对比" width="50%" /><img src="pic/Unigram2.jpg" alt="两种观点下Unigram Model的对比" width="50%" />
<p class="caption">
图 2: 两种观点下Unigram Model的对比
</p>
</div>
<p><span class="math inline">\(\quad\)</span>下面我们介绍一下先验分布、后验分布、共轭分布的概念，为我们往后的理解扫除一些障碍。</p>
<p><span class="math inline">\(\quad\)</span>共轭分布(conjugate distribution)的概率中一共涉及到三个分布：先验、似然和后验，如果由先验分布和似然分布所确定的后验分布与该先验分布属于同一种类型的分布，则该先验分布为似然分布的共轭分布，也称为共轭先验。</p>
<p><span class="math inline">\(\quad\)</span>比较绕嘴，下面从公式来理一下思路。假设变量<span class="math inline">\(x\)</span>服从分布<span class="math inline">\(P(x| \theta )\)</span>，其观测样本为<span class="math inline">\(X={x_1,x_2,...,x_m}\)</span>，参数<span class="math inline">\(θ\)</span>服从先验分布<span class="math inline">\(Π(θ)\)</span>。那么后验分布为
<span class="math display">\[P(θ|X)=\frac{Π(θ)P(X|θ)}{P(X)}\]</span></p>
<p><span class="math inline">\(\quad\)</span>如果后验分布<span class="math inline">\(P(θ|X)\)</span>与先验分布<span class="math inline">\(Π(θ)\)</span>是同种类型的分布，则称先验分布<span class="math inline">\(Π(θ)\)</span>为似然分布<span class="math inline">\(P(X|θ)\)</span>的共轭分布(直观一点就是上面那个贝叶斯公式的分子里的两个)。</p>
<p><span class="math inline">\(\quad\)</span>比较常用的几个例子有：高斯分布是高斯分布的共轭分布，Beta分布是二项分布的共轭分布，Dirichlet分布是多项分布的共轭分布。共轭分布不仅使求后验分布计算简单，更重要的是保留了先验分布的类型，使概率估计更加准确。</p>
<p><span class="math inline">\(\quad\)</span>关于Dirichlet分布是Multinomial分布的共轭分布这点，我们在此不作过多的涉及，详细推导可以参考《LDA数学八卦》这本书。但是为了下面LDA模型阐述的方便，我们交代一下Dirichlet-Multinomial的共轭关系以及Beta/Dirichlet分布的一个性质。</p>
<div id="dirichlet-multinomial的共轭关系" class="section level4">
<h4>Dirichlet-Multinomial的共轭关系</h4>
<p><span class="math inline">\(\quad\)</span>首先我们要强调的是贝叶斯参数估计的基本过程是：</p>
<center>
<strong>先验分布+数据的知识(实验后样本的情况) = 后验分布</strong>
</center>
<p><span class="math inline">\(\quad\)</span>其次我们给出两个分布的数学形式，一般形式的Dirichlet分布定义如下</p>
<p><span class="math display">\[\begin{align*}
Dir(\overrightarrow{p}|\overrightarrow{\alpha}) = 
\frac{\Gamma(\sum_{k=1}^K) \alpha_k}{\prod_{k=1}^K \Gamma(\alpha_k)}
\prod_{k=1}^K p_k^{\alpha_k - 1}
\end{align*}\]</span></p>
<p><em>注:这里的K代指进行多项分布实验的次数，或者叫维度吧。在我们UnigramModel的语境中，K = V，即K为语料库中词语总数V</em></p>
<p><span class="math inline">\(\quad\)</span>对于给定的<span class="math inline">\(\overrightarrow{p}\)</span>和<span class="math inline">\(N\)</span>，多项分布的定义为</p>
<p><span class="math display">\[\begin{align*}
Mult(\overrightarrow{n}|\overrightarrow{p},N) = \binom{N}{\overrightarrow{n}}\prod_{k = 1}^{K}p_k^{n_k}\\
\end{align*}\]</span></p>
<p><em>注:这里的K在我们UnigramModel的语境中，与上面一致。K = V，即K为语料库中词语总数V。另外，笔者查阅教科书，并未发现这种分布写法，推测这应当是一种简单写法。根据Wiki百科关于多项分布的定义，这种“分布”应该是以下的简写：</em>(<span class="math inline">\(\overrightarrow{n}\)</span>是取值离散的，只有分布律一说)</p>
<p><span class="math display">\[\begin{align*}
&amp; \overrightarrow{n} \sim Mult(\overrightarrow{p},N),其中sum(\overrightarrow{n})=N\\
&amp; P(\overrightarrow{n}) = \binom{N}{\overrightarrow{n}}\prod_{k = 1}^{K}p_k^{n_k}
\end{align*}\]</span></p>
<p><span class="math inline">\(\quad\)</span>最后，<span class="math inline">\(Mult(\overrightarrow{n}|\overrightarrow{p},N)\)</span>和<span class="math inline">\(Dir(\overrightarrow{p}|\overrightarrow{\alpha})\)</span>之间的共轭关系，不严谨地交代如下</p>
<ol style="list-style-type: decimal">
<li>以三维为例，我们要猜测参数<span class="math inline">\(\overrightarrow{p} = (p_1,p_2,p_3)\)</span>，其先验分布为<span class="math inline">\(Dir(\overrightarrow{p}|\overrightarrow{k})\)</span>；</li>
</ol>
</div>
</div>
</div>
